name: Deploy Data Warehouse to GitHub Pages

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    services:
      mysql:
        image: mysql:8.0
        env:
          MYSQL_ROOT_PASSWORD: root_password
          MYSQL_DATABASE: ProductDW
        ports:
          - 3306:3306
        options: >-
          --health-cmd="mysqladmin ping"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas numpy matplotlib seaborn scikit-learn mysql-connector-python

    - name: Wait for MySQL to be ready
      run: |
        for i in {1..30}; do
          if mysqladmin ping -h 127.0.0.1 -P 3306 -u root -proot_password --silent; then
            echo "MySQL is ready"
            break
          fi
          echo "Waiting for MySQL... ($i/30)"
          sleep 2
        done

    - name: Set up MySQL database
      run: |
        mysql -h 127.0.0.1 -P 3306 -u root -proot_password < 00_mysql_create_database.sql
        mysql -h 127.0.0.1 -P 3306 -u root -proot_password ProductDW < 01_mysql_create_dimension_tables.sql
        mysql -h 127.0.0.1 -P 3306 -u root -proot_password ProductDW < 02_mysql_create_fact_tables.sql

    - name: Import CSV data with Python
      run: |
        python -c "
        import pandas as pd
        import mysql.connector
        
        print('ðŸ“‚ Reading CSV file...')
        df = pd.read_csv('vietnamese_tiki_products_backpacks_suitcases.csv')
        print(f'âœ… Loaded {len(df)} records from CSV')
        
        # Connect to MySQL
        conn = mysql.connector.connect(
            host='127.0.0.1',
            port=3306,
            user='root',
            password='root_password',
            database='ProductDW'
        )
        
        cursor = conn.cursor()
        print('ðŸ’¾ Importing to MySQL database...')
        
        # Create staging table if not exists
        create_table_sql = '''
        CREATE TABLE IF NOT EXISTS STAGING_Products (
            row_index INT,
            id BIGINT,
            name TEXT,
            description TEXT,
            original_price DECIMAL(15,2),
            price DECIMAL(15,2),
            fulfillment_type VARCHAR(50),
            brand VARCHAR(255),
            review_count INT,
            rating_average DECIMAL(3,2),
            favourite_count INT,
            pay_later TINYINT(1),
            current_seller TEXT,
            date_created INT,
            number_of_images INT,
            vnd_cashback INT,
            has_video TINYINT(1),
            category TEXT,
            quantity_sold INT
        )
        '''
        cursor.execute(create_table_sql)
        
        # Truncate table first
        cursor.execute('TRUNCATE TABLE STAGING_Products')
        
        # Insert data
        insert_query = '''
        INSERT INTO STAGING_Products 
        (row_index, id, name, description, original_price, price, fulfillment_type, 
         brand, review_count, rating_average, favourite_count, pay_later, 
         current_seller, date_created, number_of_images, vnd_cashback, has_video, 
         category, quantity_sold)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        '''
        
        data_to_insert = []
        for index, row in df.iterrows():
            data_to_insert.append(tuple(row))
            if len(data_to_insert) >= 500:
                cursor.executemany(insert_query, data_to_insert)
                data_to_insert = []
        
        if data_to_insert:
            cursor.executemany(insert_query, data_to_insert)
        
        conn.commit()
        
        # Verify import
        cursor.execute('SELECT COUNT(*) FROM STAGING_Products')
        count = cursor.fetchone()[0]
        print(f'âœ… Imported {count} records successfully!')
        
        conn.close()
        "

    - name: Populate data warehouse
      run: |
        mysql -h 127.0.0.1 -P 3306 -u root -proot_password ProductDW < 04_mysql_populate_dimensions_fixed.sql
        mysql -h 127.0.0.1 -P 3306 -u root -proot_password ProductDW < 05_mysql_populate_fact_table_fixed.sql

    - name: Run data preprocessing
      run: |
        python data_preprocessing.py

    - name: Run OLAP and Data Mining analysis
      run: |
        python part3_olap_datamining.py

    - name: Create analysis from real database data
      run: |
        python create_real_data_analysis.py
        
    - name: Verify generated files
      run: |
        echo "Checking generated files..."
        ls -la data/clean/ || echo "data/clean directory not found"
        if [ -f "data/clean/olap_analysis.png" ]; then
          echo "âœ… OLAP analysis chart created successfully"
        else
          echo "âŒ OLAP analysis chart not found"
        fi
        if [ -f "data/clean/clustering_analysis.png" ]; then
          echo "âœ… Clustering analysis chart created successfully"  
        else
          echo "âŒ Clustering analysis chart not found"
        fi
        echo "Current directory contents:"
        find . -name "*.png" -type f | head -10
        
    - name: Commit generated charts
      run: |
        git config --global user.name 'GitHub Actions Bot'
        git config --global user.email '41898282+github-actions[bot]@users.noreply.github.com'
        
        # Check if files exist before adding
        if [ -f "data/clean/olap_analysis.png" ] && [ -f "data/clean/clustering_analysis.png" ]; then
          git add data/clean/*.png data/clean/*.csv
          
          if git diff --staged --quiet; then
            echo "[INFO] No changes to commit"
          else
            git commit -m "ðŸ¤– Auto-generate analysis charts from real data [skip ci]"
            git push origin main
            echo "[SUCCESS] Charts committed and pushed"
          fi
        else
          echo "[WARNING] Chart files not found, skipping commit"
        fi

    - name: Setup Pages
      uses: actions/configure-pages@v4
      
    - name: Upload artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: .
        
    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4